{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import get_datasets\n",
    "from its_safoos import ITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### hyper parameters that defines the structure of the model\n",
    "num_classes = 31 # ds.get_labels()\n",
    "sampled_frequencies = 129 # the number of frequency samples\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 10000  # For real training, use num_epochs=100. 10 is a test value\n",
    "# patch_size = 6  # Size of the patches to be extract from the input images\n",
    "# num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 1\n",
    "mlp_head_units = [\n",
    "    526,\n",
    "    256,\n",
    "]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n",
      "/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:159: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  hf_names = hf_datasets.list_datasets()\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = get_datasets(batch_size=batch_size, type='mfccs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model weights\n",
    "load_weights = False\n",
    "if load_weights:\n",
    "    ITS.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class GatedMlpBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inner_dim,\n",
    "        outer_dim,\n",
    "        non_linearity,\n",
    "    ):\n",
    "        super(GatedMlpBlock, self).__init__()\n",
    "        self.inner_dense_non_linear = tf.keras.layers.Dense(\n",
    "            units=inner_dim,\n",
    "            activation=non_linearity,\n",
    "        )\n",
    "        self.inner_dense_linear = tf.keras.layers.Dense(\n",
    "            units=inner_dim,\n",
    "        )\n",
    "        self.outer_dense = tf.keras.layers.Dense(\n",
    "            units=outer_dim,\n",
    "        )\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        inner_non_linear = self.inner_dense_non_linear(input_seq)\n",
    "        inner_linear = self.inner_dense_linear(input_seq)\n",
    "        multiply = inner_non_linear * inner_linear\n",
    "        return self.outer_dense(multiply)\n",
    "\n",
    "\n",
    "class RotaryPositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, theta_0, projection_dim):\n",
    "        super(RotaryPositionalEncoding, self).__init__()\n",
    "        self.indices = tf.constant([(i // 2) for i in range(projection_dim)], dtype=tf.float32)\n",
    "        self.thetas = theta_0 ** (-2 * (self.indices / projection_dim)) # thetas are of shape (projection_dim,)\n",
    "\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        # input_seq is of shape (batch, input_seq_size, projection_dim)\n",
    "        # compute the positional encoding\n",
    "        input_seq_shape = tf.shape(input_seq)\n",
    "        batch_size = input_seq_shape[0]\n",
    "        input_seq_size = input_seq_shape[1]\n",
    "        # create a vector of indices\n",
    "        seq_indices = tf.range(0, input_seq_size, 1, dtype=tf.float32) # indices are of shape (input_seq_size,)\n",
    "        # we need to create a matrix of shape (input_seq_size, projection_dim)\n",
    "        seq_indices = tf.expand_dims(seq_indices, axis=-1)\n",
    "        seq_indices = tf.tile(seq_indices, [1, tf.shape(input_seq)[2]])\n",
    "        linear_phase = seq_indices * self.thetas\n",
    "\n",
    "        # calculate the phase with consnie\n",
    "        phased_with_cos = input_seq * tf.math.cos(linear_phase)\n",
    "\n",
    "        # Rotate and multiply by [-1,1,-1,1,...] to calculate the phase with sine\n",
    "        shifted_input_seq = tf.reshape(input_seq, [batch_size, input_seq_size, -1, 2])\n",
    "        shifted_input_seq = tf.roll(shifted_input_seq, shift=1, axis=-1)\n",
    "        shifted_input_seq = shifted_input_seq * tf.constant([-1,1], dtype=tf.float32)\n",
    "        shifted_input_seq = tf.reshape(shifted_input_seq, [batch_size, input_seq_size, -1])\n",
    "        phased_with_sin =  tf.math.sin(linear_phase) * shifted_input_seq\n",
    "        \n",
    "        return phased_with_cos + phased_with_sin\n",
    "\n",
    "\n",
    "class MultiQueryAttention(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        proj_dim,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        \n",
    "        # define linear layers for key and value\n",
    "        self.key_layer = tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.value_layer = tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "        # define linear layers for query, as the number of heads\n",
    "        self.query_layers = [tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        ) for _ in range(num_heads)]\n",
    "\n",
    "        # define linear layer for output\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _compute_attn(\n",
    "        self,\n",
    "        query, # shape will be [B,S,d]\n",
    "        input_keys, # shape will be [B,T,d]\n",
    "        memory_keys, # shape will be [B,S,d]\n",
    "        input_vals, # shape will be [B,T,d]\n",
    "        memory_vals, # shape will be [B,S,d]\n",
    "    ):\n",
    "        # Assume S represents the number of memory cells and T represents the number of input cells\n",
    "        # Compute the attention weights\n",
    "        \n",
    "        # Compute the score a memory cell gives to an input cell\n",
    "        input_score = tf.matmul(query, input_keys, transpose_b=True)\n",
    "        # Shape will be [B,S,T]. This will result in a matrix,\n",
    "        # s.t. row i describes how much attention should the query i give all other input cells\n",
    "        \n",
    "        self_score = query * memory_keys\n",
    "        self_score = tf.reduce_sum(self_score, axis=-1, keepdims=True)\n",
    "        # Shape will be [B,S,1]. This will result in a vector,\n",
    "        # s.t. element i describes how much attention should the query i give to itself\n",
    "\n",
    "        # Concat self_score with input_score\n",
    "        score = tf.concat([self_score, input_score], axis=-1)\n",
    "        # Shape will be [B,S,T+1]. This will result in a matrix,\n",
    "        # s.t. row i describes how much attention should the query i give to inputs and itself\n",
    "\n",
    "        score /= tf.math.sqrt(tf.cast(tf.shape(input_keys)[-1], tf.float32))\n",
    "        attn = tf.nn.softmax(score, axis=-1)\n",
    "\n",
    "        # Break attn to [B,S,1] and [B,S,T]\n",
    "        self_attn = attn[:, :, 0:1]\n",
    "        input_attn = attn[:, :, 1:]\n",
    "\n",
    "        value_of_input = tf.matmul(input_attn, input_vals) # shape will be [B,S,d]\n",
    "        value_of_self = self_attn * memory_vals # shape will be [B,S,d]\n",
    "        return value_of_input + value_of_self\n",
    "\n",
    "\n",
    "    def call(self, input_seq, memory_cells):\n",
    "        # query_seq is of shape (batch_size, input_size, key_dim)\n",
    "        # store_seq is of shape (batch_size, store_seq, key_dim)\n",
    "        # compute the attention weights\n",
    "        ik = self.key_layer(input_seq)\n",
    "        mk = self.key_layer(memory_cells)\n",
    "        iv = self.value_layer(input_seq)\n",
    "        mv = self.value_layer(memory_cells)\n",
    "        attns = [self._compute_attn(q, ik, mk, iv, mv) for q in [layer(memory_cells) for layer in self.query_layers]]\n",
    "        concat = tf.concat(attns, axis=-1)\n",
    "        return self.output_layer(concat)\n",
    "        \n",
    "\n",
    "class StateTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        projection_dim,\n",
    "        inner_ff_dim,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(StateTransformerBlock, self).__init__()\n",
    "        # primitive properties\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = projection_dim\n",
    "        \n",
    "        # layers\n",
    "        self.attention = MultiQueryAttention(\n",
    "            num_heads=num_heads,\n",
    "            proj_dim=projection_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.add1 = tf.keras.layers.Add()\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.inner_dense = tf.keras.layers.Dense(\n",
    "            units=inner_ff_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.outer_dense = GatedMlpBlock(\n",
    "            inner_dim=inner_ff_dim,\n",
    "            outer_dim=projection_dim,\n",
    "            non_linearity=\"relu\",\n",
    "        )\n",
    "        self.ff_dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.add2 = tf.keras.layers.Add()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "\n",
    "    def call(self, state_seq, input_seq):\n",
    "        # state sequence is of shape (batch_size, num_of_state_cells, projection_dim)\n",
    "        # input sequence is of shape (batch_size, input_size, projection_dim)\n",
    "        # store_seq = tf.concat([state_seq, input_seq], axis=1)\n",
    "        attention_output = self.attention(input_seq, state_seq)\n",
    "        attention_output = self.add1([attention_output, state_seq])\n",
    "        attention_output = self.layernorm_1(attention_output)\n",
    "        inner_output = self.inner_dense(attention_output)\n",
    "        outer_output = self.outer_dense(inner_output)\n",
    "        outer_output = self.ff_dropout(outer_output)\n",
    "        outer_output = self.add2([outer_output, attention_output])\n",
    "        return self.layernorm_2(outer_output) # the output is of shape (batch_size, num_of_state_cells, projection_dim)\n",
    "    \n",
    "\n",
    "class ITSRU(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        num_state_cells,\n",
    "        projection_dim,\n",
    "        inner_ff_dim,\n",
    "        initial_state_trainability=False,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(ITSRU, self).__init__()\n",
    "\n",
    "        # Initialize the learnable initial state\n",
    "        self.initial_state = self.add_weight(\n",
    "            shape=(1, num_state_cells, projection_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=initial_state_trainability,\n",
    "            name='initial_state'\n",
    "        )\n",
    "        # State TE layers\n",
    "        self.calc_z = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.calc_r = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.calc_current_state = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "\n",
    "    def set_initial_state_trainability(self, trainable):\n",
    "        self.initial_state._trainable = trainable\n",
    "\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        # Assume that input is of size [B,T,S,D] where B is the batch size, T is the number of time steps, S is the sequence length at each timestep, and D is the feature dimension\n",
    "        # initialize the state sequence\n",
    "        batch_size = tf.shape(input_seq)[0]\n",
    "        # Use the learnable initial state, replicate it for the whole batch\n",
    "        state_t = tf.tile(self.initial_state, [batch_size, 1, 1])\n",
    "        \n",
    "        folds = tf.shape(input_seq)[1]\n",
    "        states = tf.TensorArray(\n",
    "            tf.float32,\n",
    "            dynamic_size=True,\n",
    "            size=0\n",
    "        )\n",
    "        for fold in range(folds):\n",
    "            curr_input_seq = input_seq[:, fold, :, :]\n",
    "            z = self.calc_z(state_t, curr_input_seq)\n",
    "            r = self.calc_r(state_t, curr_input_seq)\n",
    "            current_state = self.calc_current_state(r*state_t, curr_input_seq)\n",
    "            state_t = (1 - z)*state_t + z*current_state\n",
    "            states = states.write(fold, state_t)#.mark_used()\n",
    "        \n",
    "        return tf.transpose(\n",
    "            states.stack(),\n",
    "            [1, 0, 2, 3]\n",
    "        )\n",
    "\n",
    "\n",
    "class ITS(tf.keras.models.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        num_heads,\n",
    "        num_repeats,\n",
    "        num_state_cells,\n",
    "        input_seq_size,\n",
    "        projection_dim,\n",
    "        inner_ff_dim,\n",
    "        initial_state_trainability=False,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(ITS, self).__init__()\n",
    "        # the input sequence size\n",
    "        self.input_seq_size = input_seq_size\n",
    "        \n",
    "        # ITS recurrent units\n",
    "        self.encoding = tf.keras.layers.Dense(\n",
    "            units=projection_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "        self.rope = RotaryPositionalEncoding(\n",
    "            theta_0=10000,\n",
    "            projection_dim=projection_dim,\n",
    "        )\n",
    "        \n",
    "        self.itsrus = [ ITSRU(\n",
    "            num_heads=num_heads,\n",
    "            num_state_cells=num_state_cells,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            initial_state_trainability=initial_state_trainability,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        ) for _ in range(num_repeats) ]\n",
    "        \n",
    "        # self.label_token = self.add_weight(\n",
    "        #     shape=(1, 1, projection_dim),\n",
    "        #     initializer='random_normal',\n",
    "        #     trainable=initial_state_trainability,\n",
    "        #     name='initial_state'\n",
    "        # )\n",
    "        # self.mixer = StateTransformerBlock(\n",
    "        #     num_heads=num_heads,\n",
    "        #     projection_dim=projection_dim,\n",
    "        #     inner_ff_dim=inner_ff_dim,\n",
    "        #     dropout=dropout,\n",
    "        #     kernel_regularizer=kernel_regularizer,\n",
    "        # )\n",
    "\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            units=num_classes,\n",
    "            activation=\"softmax\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        # input_seq is of shape (batch_size, input_size, feature_dim).\n",
    "        # First of all, we will transform it to the shape (batch_size, folds, input_seq_size, projection_dim)\n",
    "        # Pad the input sequence to the nearest multiple of input_seq_size\n",
    "        input_seq = self.encoding(input_seq)\n",
    "        input_seq_size = input_seq.shape[1]\n",
    "        folds = tf.cast(tf.math.ceil(input_seq_size / self.input_seq_size), tf.int32)\n",
    "        final_time_steps = folds * self.input_seq_size\n",
    "        input_seq = tf.pad(\n",
    "            input_seq,\n",
    "            [[0, 0], [0, final_time_steps - input_seq_size], [0, 0]]\n",
    "        )\n",
    "        input_seq = self.rope(input_seq)\n",
    "        \n",
    "        input_seq = tf.reshape(\n",
    "            input_seq,\n",
    "            [-1, folds, self.input_seq_size, input_seq.shape[-1]]\n",
    "        )\n",
    "        # pass the input sequence through the ITSRUs\n",
    "        x = input_seq\n",
    "        for itsru in self.itsrus:\n",
    "            x = itsru(x)\n",
    "\n",
    "        # mix the states of the last timestep with the label token\n",
    "        # transform the label weight to the shape (batch_size, 1, projection_dim)\n",
    "        # label_token = tf.tile(self.label_token, [tf.shape(x)[0], 1, 1])\n",
    "        # x = self.mixer(label_token, x[:, -1, 0, :])\n",
    "        # x = tf.squeeze(x, axis=1)\n",
    "\n",
    "        return self.classifier(x[:, -1, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_transformer = ITS(\n",
    "    num_classes=31,\n",
    "    num_repeats=2,\n",
    "    num_heads=8,\n",
    "    num_state_cells=10,\n",
    "    input_seq_size=31,\n",
    "    projection_dim=32,\n",
    "    inner_ff_dim=64,\n",
    "    dropout=0.1,\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filexhjiflb7.py\", line 14, in tf__call\n        input_seq = ag__.converted_call(ag__.ld(self).rope, (ag__.ld(input_seq),), None, fscope)\n    File \"/tmp/__autograph_generated_fileba7ccgq3.py\", line 16, in tf__call\n        linear_phase = ag__.ld(seq_indices) * ag__.ld(self).thetas\n\n    ValueError: Exception encountered when calling layer 'its' (type ITS).\n    \n    in user code:\n    \n        File \"/home/zuherj/codehub/stable/active/kws/its_safoos.py\", line 344, in call  *\n            input_seq = self.rope(input_seq)\n        File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileba7ccgq3.py\", line 16, in tf__call\n            linear_phase = ag__.ld(seq_indices) * ag__.ld(self).thetas\n    \n        ValueError: Exception encountered when calling layer 'rotary_positional_encoding_4' (type RotaryPositionalEncoding).\n        \n        in user code:\n        \n            File \"/home/zuherj/codehub/stable/active/kws/its_safoos.py\", line 48, in call  *\n                linear_phase = seq_indices * self.thetas\n        \n            ValueError: Dimensions must be equal, but are 39 and 32 for '{{node its/rotary_positional_encoding_4/mul}} = Mul[T=DT_FLOAT](its/rotary_positional_encoding_4/Tile, its/rotary_positional_encoding_4/mul/y)' with input shapes: [124,39], [32].\n        \n        \n        Call arguments received by layer 'rotary_positional_encoding_4' (type RotaryPositionalEncoding):\n          • input_seq=tf.Tensor(shape=(None, 124, 39), dtype=float32)\n    \n    \n    Call arguments received by layer 'its' (type ITS):\n      • input_seq=tf.Tensor(shape=(None, 124, 39), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/its_chkpnt/its_chkpnt.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m model_checkpoint_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     10\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[1;32m     11\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m state_transformer_history \u001b[38;5;241m=\u001b[39m \u001b[43mstate_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     model_checkpoint_callback,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ],\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileile915nr.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filexhjiflb7.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     12\u001b[0m final_time_steps \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(folds) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39minput_seq_size\n\u001b[1;32m     13\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mpad, (ag__\u001b[38;5;241m.\u001b[39mld(input_seq), [[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(final_time_steps) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(input_seq_size)], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 14\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrope, (ag__\u001b[38;5;241m.\u001b[39mld(input_seq),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreshape, (ag__\u001b[38;5;241m.\u001b[39mld(input_seq), [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(folds), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39minput_seq_size, ag__\u001b[38;5;241m.\u001b[39mld(input_seq)\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(input_seq)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileba7ccgq3.py:16\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     14\u001b[0m seq_indices \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dims, (ag__\u001b[38;5;241m.\u001b[39mld(seq_indices),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[1;32m     15\u001b[0m seq_indices \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtile, (ag__\u001b[38;5;241m.\u001b[39mld(seq_indices), [\u001b[38;5;241m1\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(input_seq),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m2\u001b[39m]]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 16\u001b[0m linear_phase \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(seq_indices) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mthetas\n\u001b[1;32m     17\u001b[0m phased_with_cos \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(input_seq) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mcos, (ag__\u001b[38;5;241m.\u001b[39mld(linear_phase),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     18\u001b[0m shifted_input_seq \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreshape, (ag__\u001b[38;5;241m.\u001b[39mld(input_seq), [ag__\u001b[38;5;241m.\u001b[39mld(batch_size), ag__\u001b[38;5;241m.\u001b[39mld(input_seq_size), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filexhjiflb7.py\", line 14, in tf__call\n        input_seq = ag__.converted_call(ag__.ld(self).rope, (ag__.ld(input_seq),), None, fscope)\n    File \"/tmp/__autograph_generated_fileba7ccgq3.py\", line 16, in tf__call\n        linear_phase = ag__.ld(seq_indices) * ag__.ld(self).thetas\n\n    ValueError: Exception encountered when calling layer 'its' (type ITS).\n    \n    in user code:\n    \n        File \"/home/zuherj/codehub/stable/active/kws/its_safoos.py\", line 344, in call  *\n            input_seq = self.rope(input_seq)\n        File \"/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_fileba7ccgq3.py\", line 16, in tf__call\n            linear_phase = ag__.ld(seq_indices) * ag__.ld(self).thetas\n    \n        ValueError: Exception encountered when calling layer 'rotary_positional_encoding_4' (type RotaryPositionalEncoding).\n        \n        in user code:\n        \n            File \"/home/zuherj/codehub/stable/active/kws/its_safoos.py\", line 48, in call  *\n                linear_phase = seq_indices * self.thetas\n        \n            ValueError: Dimensions must be equal, but are 39 and 32 for '{{node its/rotary_positional_encoding_4/mul}} = Mul[T=DT_FLOAT](its/rotary_positional_encoding_4/Tile, its/rotary_positional_encoding_4/mul/y)' with input shapes: [124,39], [32].\n        \n        \n        Call arguments received by layer 'rotary_positional_encoding_4' (type RotaryPositionalEncoding):\n          • input_seq=tf.Tensor(shape=(None, 124, 39), dtype=float32)\n    \n    \n    Call arguments received by layer 'its' (type ITS):\n      • input_seq=tf.Tensor(shape=(None, 124, 39), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "state_transformer.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "model_path = \"./models/its_chkpnt/its_chkpnt.ckpt\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "state_transformer_history = state_transformer.fit(\n",
    "    train,\n",
    "    validation_data=valid,\n",
    "    epochs=21,\n",
    "    # callbacks=[\n",
    "    #     model_checkpoint_callback,\n",
    "    # ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both models TCResNet and StateTransformer for 30 epochs and graph the accuracy results\n",
    "import matplotlib.pyplot as plt\n",
    "results = {\n",
    "\n",
    "}\n",
    "for num_state_cells in [1, 4, 8, 12]:\n",
    "    state_transformer = ITS(\n",
    "        num_classes=31,\n",
    "        num_repeats=2,\n",
    "        num_heads=8,\n",
    "        num_state_cells=num_state_cells,\n",
    "        input_seq_size=31,\n",
    "        projection_dim=32,\n",
    "        inner_ff_dim=64,\n",
    "        dropout=0.1,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )\n",
    "\n",
    "    state_transformer.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    model_path = \"./models/its_chkpnt/its_chkpnt.ckpt\"\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_weights_only=True,\n",
    "        save_freq=\"epoch\",\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    state_transformer_history = state_transformer.fit(\n",
    "        train,\n",
    "        validation_data=valid,\n",
    "        epochs=21,\n",
    "        # callbacks=[\n",
    "        #     model_checkpoint_callback,\n",
    "        # ],\n",
    "    )\n",
    "    results[num_state_cells] = state_transformer_history.history['val_accuracy']\n",
    "\n",
    "# write results to a csv file\n",
    "import csv\n",
    "with open('results.csv', 'w') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    for key, values in results.items():\n",
    "        csv_writer.writerow([key] + values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('results.csv', 'r') as f:\n",
    "    model_names = []\n",
    "    data = []\n",
    "    first_row = True\n",
    "    csv_reader = csv.reader(f)\n",
    "    for row in csv_reader:\n",
    "        if first_row:\n",
    "            model_names = row\n",
    "            first_row = False\n",
    "            continue\n",
    "        else:\n",
    "            data.append(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_lines():\n",
    "    model_names = []\n",
    "    data = []\n",
    "    with open(\"results.csv\", 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                model_names = row\n",
    "                first_row = False\n",
    "            else:\n",
    "                data.append(row)\n",
    "\n",
    "    return model_names, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names, data = get_file_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_names)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 1\n",
    "state_cells = 4\n",
    "model_names.index(f\"r={repeats},s={state_cells}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('results.csv', 'w')\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow(list(product([1, 2, 3], [4, 5, 6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
