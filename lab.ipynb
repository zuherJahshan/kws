{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 09:34:56.038755: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-17 09:34:56.062579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-17 09:34:56.406584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_datasets\n",
    "from its_lru_v2 import ITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### hyper parameters that defines the structure of the model\n",
    "num_classes = 31 # ds.get_labels()\n",
    "sampled_frequencies = 129 # the number of frequency samples\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 10000  # For real training, use num_epochs=100. 10 is a test value\n",
    "# patch_size = 6  # Size of the patches to be extract from the input images\n",
    "# num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 1\n",
    "mlp_head_units = [\n",
    "    526,\n",
    "    256,\n",
    "]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n",
      "/home/zuherj/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:159: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  hf_names = hf_datasets.list_datasets()\n",
      "2024-03-17 09:35:27.770468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 09:35:27.785171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 09:35:27.785295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 09:35:27.786177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 09:35:27.786260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 09:35:27.786311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 09:35:28.131257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 09:35:28.131373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 09:35:28.131429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-17 09:35:28.131478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22325 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = get_datasets(batch_size=batch_size, type='mfccs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model weights\n",
    "load_weights = False\n",
    "if load_weights:\n",
    "    ITS.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class GatedMlpBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inner_dim,\n",
    "        outer_dim,\n",
    "        non_linearity,\n",
    "    ):\n",
    "        super(GatedMlpBlock, self).__init__()\n",
    "        self.inner_dense_non_linear = tf.keras.layers.Dense(\n",
    "            units=inner_dim,\n",
    "            activation=non_linearity,\n",
    "        )\n",
    "        self.inner_dense_linear = tf.keras.layers.Dense(\n",
    "            units=inner_dim,\n",
    "        )\n",
    "        self.outer_dense = tf.keras.layers.Dense(\n",
    "            units=outer_dim,\n",
    "        )\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        inner_non_linear = self.inner_dense_non_linear(input_seq)\n",
    "        inner_linear = self.inner_dense_linear(input_seq)\n",
    "        multiply = inner_non_linear * inner_linear\n",
    "        return self.outer_dense(multiply)\n",
    "\n",
    "\n",
    "class MultiQueryAttention(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        proj_dim,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        \n",
    "        # define linear layers for key and value\n",
    "        self.key_layer = tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.value_layer = tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "        # define linear layers for query, as the number of heads\n",
    "        self.query_layers = [tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        ) for _ in range(num_heads)]\n",
    "\n",
    "        # define linear layer for output\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _compute_attn(\n",
    "        self,\n",
    "        query, # shape will be [B,S,d]\n",
    "        input_keys, # shape will be [B,T,d]\n",
    "        memory_keys, # shape will be [B,S,d]\n",
    "        input_vals, # shape will be [B,T,d]\n",
    "        memory_vals, # shape will be [B,S,d]\n",
    "    ):\n",
    "        # Assume S represents the number of memory cells and T represents the number of input cells\n",
    "        # Compute the attention weights\n",
    "        \n",
    "        # Compute the score a memory cell gives to an input cell\n",
    "        input_score = tf.matmul(query, input_keys, transpose_b=True)\n",
    "        # Shape will be [B,S,T]. This will result in a matrix,\n",
    "        # s.t. row i describes how much attention should the query i give all other input cells\n",
    "        \n",
    "        self_score = query * memory_keys\n",
    "        self_score = tf.reduce_sum(self_score, axis=-1, keepdims=True)\n",
    "        # Shape will be [B,S,1]. This will result in a vector,\n",
    "        # s.t. element i describes how much attention should the query i give to itself\n",
    "\n",
    "        # Concat self_score with input_score\n",
    "        score = tf.concat([self_score, input_score], axis=-1)\n",
    "        # Shape will be [B,S,T+1]. This will result in a matrix,\n",
    "        # s.t. row i describes how much attention should the query i give to inputs and itself\n",
    "\n",
    "        score /= tf.math.sqrt(tf.cast(tf.shape(input_keys)[-1], tf.float32))\n",
    "        attn = tf.nn.softmax(score, axis=-1)\n",
    "\n",
    "        # Break attn to [B,S,1] and [B,S,T]\n",
    "        self_attn = attn[:, :, 0:1]\n",
    "        input_attn = attn[:, :, 1:]\n",
    "\n",
    "        value_of_input = tf.matmul(input_attn, input_vals) # shape will be [B,S,d]\n",
    "        value_of_self = self_attn * memory_vals # shape will be [B,S,d]\n",
    "        return value_of_input + value_of_self\n",
    "\n",
    "\n",
    "    def call(self, input_seq, memory_cells):\n",
    "        # query_seq is of shape (batch_size, input_size, key_dim)\n",
    "        # store_seq is of shape (batch_size, store_seq, key_dim)\n",
    "        # compute the attention weights\n",
    "        ik = self.key_layer(input_seq)\n",
    "        mk = self.key_layer(memory_cells)\n",
    "        iv = self.value_layer(input_seq)\n",
    "        mv = self.value_layer(memory_cells)\n",
    "        attns = [self._compute_attn(q, ik, mk, iv, mv) for q in [layer(memory_cells) for layer in self.query_layers]]\n",
    "        concat = tf.concat(attns, axis=-1)\n",
    "        return self.output_layer(concat)\n",
    "        \n",
    "\n",
    "class StateTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        projection_dim,\n",
    "        inner_ff_dim,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(StateTransformerBlock, self).__init__()\n",
    "        # primitive properties\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = projection_dim\n",
    "        \n",
    "        # layers\n",
    "        self.attention = MultiQueryAttention(\n",
    "            num_heads=num_heads,\n",
    "            proj_dim=projection_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.add1 = tf.keras.layers.Add()\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.inner_dense = tf.keras.layers.Dense(\n",
    "            units=inner_ff_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.outer_dense = GatedMlpBlock(\n",
    "            inner_dim=inner_ff_dim,\n",
    "            outer_dim=projection_dim,\n",
    "            non_linearity=\"relu\",\n",
    "        )\n",
    "        self.ff_dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.add2 = tf.keras.layers.Add()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "\n",
    "    def call(self, state_seq, input_seq):\n",
    "        # state sequence is of shape (batch_size, num_of_state_cells, projection_dim)\n",
    "        # input sequence is of shape (batch_size, input_size, projection_dim)\n",
    "        # store_seq = tf.concat([state_seq, input_seq], axis=1)\n",
    "        attention_output = self.attention(input_seq, state_seq)\n",
    "        attention_output = self.add1([attention_output, state_seq])\n",
    "        attention_output = self.layernorm_1(attention_output)\n",
    "        inner_output = self.inner_dense(attention_output)\n",
    "        outer_output = self.outer_dense(inner_output)\n",
    "        outer_output = self.ff_dropout(outer_output)\n",
    "        outer_output = self.add2([outer_output, attention_output])\n",
    "        return self.layernorm_2(outer_output) # the output is of shape (batch_size, num_of_state_cells, projection_dim)\n",
    "    \n",
    "\n",
    "class ITSRU(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        num_state_cells,\n",
    "        projection_dim,\n",
    "        inner_ff_dim,\n",
    "        initial_state_trainability=False,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(ITSRU, self).__init__()\n",
    "\n",
    "        self.encoding = tf.keras.layers.Dense(\n",
    "            units=projection_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        # Initialize the learnable initial state\n",
    "        self.initial_state = self.add_weight(\n",
    "            shape=(1, num_state_cells, projection_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=initial_state_trainability,\n",
    "            name='initial_state'\n",
    "        )\n",
    "        # State TE layers\n",
    "        self.calc_z = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.calc_r = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.calc_current_state = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "\n",
    "    def set_initial_state_trainability(self, trainable):\n",
    "        self.initial_state._trainable = trainable\n",
    "\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        # Assume that input is of size [B,T,S,D] where B is the batch size, T is the number of time steps, S is the sequence length at each timestep, and D is the feature dimension\n",
    "        input_seq = self.encoding(input_seq)\n",
    "        # initialize the state sequence\n",
    "        batch_size = tf.shape(input_seq)[0]\n",
    "        # Use the learnable initial state, replicate it for the whole batch\n",
    "        state_t = tf.tile(self.initial_state, [batch_size, 1, 1])\n",
    "        \n",
    "        folds = tf.shape(input_seq)[1]\n",
    "        states = tf.TensorArray(\n",
    "            tf.float32,\n",
    "            dynamic_size=True,\n",
    "            size=0\n",
    "        )\n",
    "        for fold in range(folds):\n",
    "            curr_input_seq = input_seq[:, fold, :, :]\n",
    "            z = self.calc_z(state_t, curr_input_seq)\n",
    "            r = self.calc_r(state_t, curr_input_seq)\n",
    "            current_state = self.calc_current_state(r*state_t, curr_input_seq)\n",
    "            state_t = (1 - z)*state_t + z*current_state\n",
    "            states = states.write(fold, state_t)#.mark_used()\n",
    "        \n",
    "        return tf.transpose(\n",
    "            states.stack(),\n",
    "            [1, 0, 2, 3]\n",
    "        )\n",
    "\n",
    "\n",
    "class ITS(tf.keras.models.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        num_heads,\n",
    "        num_repeats,\n",
    "        num_state_cells,\n",
    "        input_seq_size,\n",
    "        projection_dim,\n",
    "        inner_ff_dim,\n",
    "        initial_state_trainability=False,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(ITS, self).__init__()\n",
    "        # the input sequence size\n",
    "        self.input_seq_size = input_seq_size\n",
    "        \n",
    "        # ITS recurrent units\n",
    "        self.itsrus = [ ITSRU(\n",
    "            num_heads=num_heads,\n",
    "            num_state_cells=num_state_cells,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            initial_state_trainability=initial_state_trainability,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        ) for _ in range(num_repeats) ]\n",
    "        \n",
    "        self.label_token = self.add_weight(\n",
    "            shape=(1, 1, projection_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=initial_state_trainability,\n",
    "            name='initial_state'\n",
    "        )\n",
    "        self.mixer = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            units=num_classes,\n",
    "            activation=\"softmax\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        # input_seq is of shape (batch_size, input_size, feature_dim).\n",
    "        # First of all, we will transform it to the shape (batch_size, folds, input_seq_size, projection_dim)\n",
    "        # Pad the input sequence to the nearest multiple of input_seq_size\n",
    "        input_seq_size = input_seq.shape[1]\n",
    "        folds = tf.cast(tf.math.ceil(input_seq_size / self.input_seq_size), tf.int32)\n",
    "        final_time_steps = folds * self.input_seq_size\n",
    "        input_seq = tf.pad(\n",
    "            input_seq,\n",
    "            [[0, 0], [0, final_time_steps - input_seq_size], [0, 0]]\n",
    "        )\n",
    "        \n",
    "        input_seq = tf.reshape(\n",
    "            input_seq,\n",
    "            [-1, folds, self.input_seq_size, input_seq.shape[-1]]\n",
    "        )\n",
    "        # pass the input sequence through the ITSRUs\n",
    "        x = input_seq\n",
    "        for itsru in self.itsrus:\n",
    "            x = itsru(x)\n",
    "\n",
    "        # mix the states of the last timestep with the label token\n",
    "        # transform the label weight to the shape (batch_size, 1, projection_dim)\n",
    "        # label_token = tf.tile(self.label_token, [tf.shape(x)[0], 1, 1])\n",
    "        # x = self.mixer(label_token, x[:, -1, 0, :])\n",
    "        # x = tf.squeeze(x, axis=1)\n",
    "\n",
    "        return self.classifier(x[:, -1, 0, :])\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_transformer = ITS(\n",
    "    num_classes=31,\n",
    "    num_repeats=2,\n",
    "    num_heads=8,\n",
    "    num_state_cells=10,\n",
    "    input_seq_size=31,\n",
    "    projection_dim=32,\n",
    "    inner_ff_dim=64,\n",
    "    dropout=0.1,\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 32])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n",
      "TensorShape([None, 10, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-17 09:45:55.673672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-17 09:45:55.760224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa9cc00fcd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-17 09:45:55.760238: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A5000, Compute Capability 8.6\n",
      "2024-03-17 09:45:55.763221: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-17 09:45:55.857654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-03-17 09:45:55.920146: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/its_chkpnt/its_chkpnt.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m model_checkpoint_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     10\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[1;32m     11\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m state_transformer_history \u001b[38;5;241m=\u001b[39m \u001b[43mstate_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     model_checkpoint_callback,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ],\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:890\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    893\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_function_spec  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    894\u001b[0m       \u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    895\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/kws/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_transformer.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "model_path = \"./models/its_chkpnt/its_chkpnt.ckpt\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "state_transformer_history = state_transformer.fit(\n",
    "    train,\n",
    "    validation_data=valid,\n",
    "    epochs=21,\n",
    "    # callbacks=[\n",
    "    #     model_checkpoint_callback,\n",
    "    # ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both models TCResNet and StateTransformer for 30 epochs and graph the accuracy results\n",
    "import matplotlib.pyplot as plt\n",
    "results = {\n",
    "\n",
    "}\n",
    "for num_state_cells in [1, 4, 8, 12]:\n",
    "    state_transformer = ITS(\n",
    "        num_classes=31,\n",
    "        num_repeats=2,\n",
    "        num_heads=8,\n",
    "        num_state_cells=num_state_cells,\n",
    "        input_seq_size=31,\n",
    "        projection_dim=32,\n",
    "        inner_ff_dim=64,\n",
    "        dropout=0.1,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )\n",
    "\n",
    "    state_transformer.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    model_path = \"./models/its_chkpnt/its_chkpnt.ckpt\"\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_weights_only=True,\n",
    "        save_freq=\"epoch\",\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    state_transformer_history = state_transformer.fit(\n",
    "        train,\n",
    "        validation_data=valid,\n",
    "        epochs=21,\n",
    "        # callbacks=[\n",
    "        #     model_checkpoint_callback,\n",
    "        # ],\n",
    "    )\n",
    "    results[num_state_cells] = state_transformer_history.history['val_accuracy']\n",
    "\n",
    "# write results to a csv file\n",
    "import csv\n",
    "with open('results.csv', 'w') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    for key, values in results.items():\n",
    "        csv_writer.writerow([key] + values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('results.csv', 'r') as f:\n",
    "    model_names = []\n",
    "    data = []\n",
    "    first_row = True\n",
    "    csv_reader = csv.reader(f)\n",
    "    for row in csv_reader:\n",
    "        if first_row:\n",
    "            model_names = row\n",
    "            first_row = False\n",
    "            continue\n",
    "        else:\n",
    "            data.append(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_lines():\n",
    "    model_names = []\n",
    "    data = []\n",
    "    with open(\"results.csv\", 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        first_row = True\n",
    "        for row in reader:\n",
    "            if first_row:\n",
    "                model_names = row\n",
    "                first_row = False\n",
    "            else:\n",
    "                data.append(row)\n",
    "\n",
    "    return model_names, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names, data = get_file_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r=1,s=1', 'r=1,s=4', 'r=1,s=10', 'r=1,s=15', 'r=2,s=1', 'r=2,s=4', 'r=2,s=10', 'r=2,s=15', 'r=4,s=1', 'r=4,s=4', 'r=4,s=10', 'r=4,s=15']\n",
      "[['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]\n"
     ]
    }
   ],
   "source": [
    "print(model_names)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeats = 1\n",
    "state_cells = 4\n",
    "model_names.index(f\"r={repeats},s={state_cells}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('results.csv', 'w')\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow(list(product([1, 2, 3], [4, 5, 6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
