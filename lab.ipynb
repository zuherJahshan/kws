{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import get_datasets\n",
    "from its_lru import ITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### hyper parameters that defines the structure of the model\n",
    "num_classes = 31 # ds.get_labels()\n",
    "sampled_frequencies = 129 # the number of frequency samples\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 10000  # For real training, use num_epochs=100. 10 is a test value\n",
    "# patch_size = 6  # Size of the patches to be extract from the input images\n",
    "# num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 1\n",
    "mlp_head_units = [\n",
    "    526,\n",
    "    256,\n",
    "]  # Size of the dense layers of the final classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = get_datasets(batch_size=batch_size, type='mfccs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model weights\n",
    "load_weights = False\n",
    "if load_weights:\n",
    "    ITS.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class GatedMlpBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inner_dim,\n",
    "        outer_dim,\n",
    "        non_linearity,\n",
    "    ):\n",
    "        super(GatedMlpBlock, self).__init__()\n",
    "        self.inner_dense_non_linear = tf.keras.layers.Dense(\n",
    "            units=inner_dim,\n",
    "            activation=non_linearity,\n",
    "        )\n",
    "        self.inner_dense_linear = tf.keras.layers.Dense(\n",
    "            units=inner_dim,\n",
    "        )\n",
    "        self.outer_dense = tf.keras.layers.Dense(\n",
    "            units=outer_dim,\n",
    "        )\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        inner_non_linear = self.inner_dense_non_linear(input_seq)\n",
    "        inner_linear = self.inner_dense_linear(input_seq)\n",
    "        multiply = inner_non_linear * inner_linear\n",
    "        return self.outer_dense(multiply)\n",
    "\n",
    "\n",
    "class MultiQueryAttention(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        proj_dim,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        \n",
    "        # define linear layers for key and value\n",
    "        self.key_layer = tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.value_layer = tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "        # define linear layers for query, as the number of heads\n",
    "        self.query_layers = [tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        ) for _ in range(num_heads)]\n",
    "\n",
    "        # define linear layer for output\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=proj_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _compute_attn(self, k, q , v):\n",
    "        # K, V are of shape [B,S,d]\n",
    "        # Q is of shape [B,T,d]\n",
    "        # compute the attention weights\n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        score /= tf.math.sqrt(tf.cast(tf.shape(k)[-1], tf.float32))\n",
    "        attn = tf.nn.softmax(score, axis=-1)\n",
    "        return tf.matmul(attn, v)\n",
    "\n",
    "\n",
    "    def call(self, query_seq, store_seq):\n",
    "        # query_seq is of shape (batch_size, input_size, key_dim)\n",
    "        # store_seq is of shape (batch_size, store_seq, key_dim)\n",
    "        # compute the attention weights\n",
    "        k = self.key_layer(store_seq)\n",
    "        v = self.value_layer(store_seq)\n",
    "        attns = [self._compute_attn(k, q, v) for q in [layer(query_seq) for layer in self.query_layers]]\n",
    "        concat = tf.concat(attns, axis=-1)\n",
    "        return self.output_layer(concat)\n",
    "        \n",
    "\n",
    "class StateTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        projection_dim,\n",
    "        inner_ff_dim,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(StateTransformerBlock, self).__init__()\n",
    "        # primitive properties\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = projection_dim\n",
    "        \n",
    "        # layers\n",
    "        self.attention = MultiQueryAttention(\n",
    "            num_heads=num_heads,\n",
    "            proj_dim=projection_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.add1 = tf.keras.layers.Add()\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.inner_dense = tf.keras.layers.Dense(\n",
    "            units=inner_ff_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.outer_dense = GatedMlpBlock(\n",
    "            inner_dim=inner_ff_dim,\n",
    "            outer_dim=projection_dim,\n",
    "            non_linearity=\"relu\",\n",
    "        )\n",
    "        self.ff_dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.add2 = tf.keras.layers.Add()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "\n",
    "    def call(self, state_seq, input_seq):\n",
    "        # state sequence is of shape (batch_size, num_of_state_cells, projection_dim)\n",
    "        # input sequence is of shape (batch_size, input_size, projection_dim)\n",
    "        store_seq = tf.concat([state_seq, input_seq], axis=1)\n",
    "        attention_output = self.attention(state_seq, store_seq)\n",
    "        attention_output = self.add1([attention_output, state_seq])\n",
    "        attention_output = self.layernorm_1(attention_output)\n",
    "        inner_output = self.inner_dense(attention_output)\n",
    "        outer_output = self.outer_dense(inner_output)\n",
    "        outer_output = self.ff_dropout(outer_output)\n",
    "        outer_output = self.add2([outer_output, attention_output])\n",
    "        return self.layernorm_2(outer_output) # the output is of shape (batch_size, num_of_state_cells, projection_dim)\n",
    "    \n",
    "\n",
    "class ITSRU(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads,\n",
    "        num_state_cells,\n",
    "        projection_dim,\n",
    "        inner_ff_dim,\n",
    "        initial_state_trainability=False,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(ITSRU, self).__init__()\n",
    "\n",
    "        self.encoding = tf.keras.layers.Dense(\n",
    "            units=projection_dim,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        # Initialize the learnable initial state\n",
    "        self.initial_state = self.add_weight(\n",
    "            shape=(1, num_state_cells, projection_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=initial_state_trainability,\n",
    "            name='initial_state'\n",
    "        )\n",
    "        # State TE layers\n",
    "        self.calc_z = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.calc_r = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "        self.calc_current_state = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "\n",
    "    def set_initial_state_trainability(self, trainable):\n",
    "        self.initial_state._trainable = trainable\n",
    "\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        # Assume that input is of size [B,T,S,D] where B is the batch size, T is the number of time steps, S is the sequence length at each timestep, and D is the feature dimension\n",
    "        input_seq = self.encoding(input_seq)\n",
    "        # initialize the state sequence\n",
    "        batch_size = tf.shape(input_seq)[0]\n",
    "        # Use the learnable initial state, replicate it for the whole batch\n",
    "        state_t = tf.tile(self.initial_state, [batch_size, 1, 1])\n",
    "        \n",
    "        folds = tf.shape(input_seq)[1]\n",
    "        states = tf.TensorArray(\n",
    "            tf.float32,\n",
    "            dynamic_size=True,\n",
    "            size=0\n",
    "        )\n",
    "        for fold in range(folds):\n",
    "            curr_input_seq = input_seq[:, fold, :, :]\n",
    "            z = self.calc_z(state_t, curr_input_seq)\n",
    "            r = self.calc_r(state_t, curr_input_seq)\n",
    "            current_state = self.calc_current_state(r*state_t, curr_input_seq)\n",
    "            state_t = (1 - z)*state_t + z*current_state\n",
    "            states = states.write(fold, state_t)#.mark_used()\n",
    "        \n",
    "        return tf.transpose(\n",
    "            states.stack(),\n",
    "            [1, 0, 2, 3]\n",
    "        )\n",
    "\n",
    "\n",
    "class ITS(tf.keras.models.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        num_heads,\n",
    "        num_repeats,\n",
    "        num_state_cells,\n",
    "        input_seq_size,\n",
    "        projection_dim,\n",
    "        inner_ff_dim,\n",
    "        initial_state_trainability=False,\n",
    "        dropout=0.0,\n",
    "        kernel_regularizer=None,\n",
    "    ):\n",
    "        super(ITS, self).__init__()\n",
    "        # the input sequence size\n",
    "        self.input_seq_size = input_seq_size\n",
    "        \n",
    "        # ITS recurrent units\n",
    "        self.itsrus = [ ITSRU(\n",
    "            num_heads=num_heads,\n",
    "            num_state_cells=num_state_cells,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            initial_state_trainability=initial_state_trainability,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        ) for _ in range(num_repeats) ]\n",
    "        \n",
    "        self.label_token = self.add_weight(\n",
    "            shape=(1, 1, projection_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=initial_state_trainability,\n",
    "            name='initial_state'\n",
    "        )\n",
    "        self.mixer = StateTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            projection_dim=projection_dim,\n",
    "            inner_ff_dim=inner_ff_dim,\n",
    "            dropout=dropout,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "        )\n",
    "\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            units=num_classes,\n",
    "            activation=\"softmax\",\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, input_seq):\n",
    "        # input_seq is of shape (batch_size, input_size, feature_dim).\n",
    "        # First of all, we will transform it to the shape (batch_size, folds, input_seq_size, projection_dim)\n",
    "        # Pad the input sequence to the nearest multiple of input_seq_size\n",
    "        input_seq_size = input_seq.shape[1]\n",
    "        folds = tf.cast(tf.math.ceil(input_seq_size / self.input_seq_size), tf.int32)\n",
    "        final_time_steps = folds * self.input_seq_size\n",
    "        input_seq = tf.pad(\n",
    "            input_seq,\n",
    "            [[0, 0], [0, final_time_steps - input_seq_size], [0, 0]]\n",
    "        )\n",
    "        \n",
    "        input_seq = tf.reshape(\n",
    "            input_seq,\n",
    "            [-1, folds, self.input_seq_size, input_seq.shape[-1]]\n",
    "        )\n",
    "        # pass the input sequence through the ITSRUs\n",
    "        x = input_seq\n",
    "        for itsru in self.itsrus:\n",
    "            x = itsru(x)\n",
    "\n",
    "        # mix the states of the last timestep with the label token\n",
    "        # transform the label weight to the shape (batch_size, 1, projection_dim)\n",
    "        label_token = tf.tile(self.label_token, [tf.shape(x)[0], 1, 1])\n",
    "        x = self.mixer(label_token, x[:, -1, :, :])\n",
    "        x = tf.squeeze(x, axis=1)\n",
    "\n",
    "        return self.classifier(x)\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its = ITS(\n",
    "#     num_classes=31,\n",
    "#     num_repeats=3,\n",
    "#     num_heads=8,\n",
    "#     num_state_cells=10,\n",
    "#     input_seq_size=20,\n",
    "#     projection_dim=32,\n",
    "#     inner_ff_dim=64,\n",
    "#     dropout=0.1,\n",
    "#     kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "# )\n",
    "# for x, y in train:\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     its(x)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "215/799 [=======>......................] - ETA: 59s - loss: 22.6080 - accuracy: 0.0362"
     ]
    }
   ],
   "source": [
    "# Run both models TCResNet and StateTransformer for 30 epochs and graph the accuracy results\n",
    "import matplotlib.pyplot as plt\n",
    "results = {\n",
    "\n",
    "}\n",
    "for num_state_cells in [1, 4, 8, 12]:\n",
    "    state_transformer = ITS(\n",
    "        num_classes=31,\n",
    "        num_repeats=2,\n",
    "        num_heads=8,\n",
    "        num_state_cells=num_state_cells,\n",
    "        input_seq_size=31,\n",
    "        projection_dim=32,\n",
    "        inner_ff_dim=64,\n",
    "        dropout=0.1,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "    )\n",
    "\n",
    "    state_transformer.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    model_path = \"./models/its_chkpnt/its_chkpnt.ckpt\"\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_weights_only=True,\n",
    "        save_freq=\"epoch\",\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    state_transformer_history = state_transformer.fit(\n",
    "        train,\n",
    "        validation_data=valid,\n",
    "        epochs=21,\n",
    "        # callbacks=[\n",
    "        #     model_checkpoint_callback,\n",
    "        # ],\n",
    "    )\n",
    "    results[num_state_cells] = state_transformer_history.history['val_accuracy']\n",
    "\n",
    "# write results to a csv file\n",
    "# import csv\n",
    "# with open('results.csv', 'w') as f:\n",
    "#     csv_writer = csv.writer(f)\n",
    "#     for key, values in results.items():\n",
    "#         csv_writer.writerow([key] + values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
